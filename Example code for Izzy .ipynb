{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc7203a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:18\u001b[0;36m\u001b[0m\n\u001b[0;31m    reqs = requests.get(url)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Extract all pages from a site and it appends to a list called \"url\". This code works when just asked to print the output (commented out in code) but append is not working yet. \n",
    "\n",
    "#Define the function ScrapingWebsite, then loop for all items in url\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### function\n",
    "# extract all pages from a company's home website\n",
    "def getPages(url):\n",
    "\t# a function to extract all links in the website (not working yet)\n",
    "\t# parameter: url, a company's home website address\n",
    "\t# return: a list of links within the website\n",
    "\n",
    "\turl = 'https://hannahespy.com/'\n",
    "    reqs = requests.get(url)\n",
    "\tsoup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "\turls = []\n",
    "\tfor link in soup.find_all('a'):\n",
    "\t\t#print(link.get('herf'))\n",
    "\t\turls.append(link.get('herf'))\n",
    "\n",
    "\treturn urls\n",
    "\n",
    "page_list = getPages(\"https://hannahespy.com\")  # change the url \n",
    "\n",
    "print(page_list)\n",
    "\n",
    "#Step 2: Extracting just the text of a single web page (loop for extracting all links not yet working):\n",
    "\n",
    "def scrapingWebpage(url, file_name):\n",
    "\t# get all text in a page\n",
    "\t# parameters: url, a string of website address; file_name: a string defined by you, ideally with file address (e.g. c:\\Users\\yorin\\Desktop\\)\n",
    "\t# return: None; output: text files of website texts to your assigned folders\n",
    "\n",
    "\tprint(url)\n",
    "\tr = requests.get(url)\n",
    "\n",
    "\tencoding = r.encoding if 'charset' in r.headers.get('content-type', '').lower() else None\n",
    "\tparser = 'html.parser'  # or lxml or html5lib\n",
    "\tsoup = BeautifulSoup(r.content, parser, from_encoding=encoding)\n",
    "\n",
    "\ttext = soup.get_text()\n",
    "\n",
    "\n",
    "\tf = open(str(file_name)+\".txt\", \"w\", encoding = encoding)         # Creating html_text.txt File\n",
    "\n",
    "\tfor line in text:\n",
    "\t\tf.write(line)\n",
    "\n",
    "\tf.close()\n",
    "\n",
    "# for item in page_list:\n",
    "# \tscrapingWebpage(item)\n",
    "\n",
    "\n",
    "scrapingWebpage(\"https://hannahespy.com\",r\"c:\\Users\\hwespy\\Desktop\\scrapetest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dd36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
